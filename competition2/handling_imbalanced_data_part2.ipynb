{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "handling_imbalanced_data_part2.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "636Kuxc_ymLD",
        "colab_type": "text"
      },
      "source": [
        "# Handling Imbalanced Data with SMOTE and Near Miss Algorithm in Python\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eST21SLHzDm3",
        "colab_type": "text"
      },
      "source": [
        "## What is imbalanced data?\n",
        "\n",
        "- __Imbalanced Data Distribution__, generally happens when observations in _one of the class_ are much higher or lower than the other classes. \n",
        "\n",
        "- As Machine Learning algorithms tend to increase accuracy by reducing the error, they do not consider the class distribution. \n",
        "\n",
        "- This problem is prevalent in examples such as __Fraud Detection, Anomaly Detection, Facial recognition__ etc."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nqExKOdOzJPE",
        "colab_type": "text"
      },
      "source": [
        "## Why is it a problem?\n",
        "\n",
        "- Standard ML techniques, such as Decision Tree and Logistic Regression, have a bias towards the __majority class__, and they tend to ignore the minority class. \n",
        "\n",
        "  - They tend only to predict the majority class, hence, having _major misclassification_ of the minority class in comparison with the majority class. \n",
        "  \n",
        "- In more technical words, if we have imbalanced data distribution in our dataset then our model becomes more prone to the case when minority class has negligible or very __lesser__ recall.\n",
        "\n",
        "  - where you end up with models with high accuracy but biased"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DvKC2d84z0j1",
        "colab_type": "text"
      },
      "source": [
        "## How to handle imbalanced data in theory?\n",
        "\n",
        "- In general, there are two techniques: undersampling and oversampling\n",
        "  - suppose we have two classes (`0: 200, 1:1000`)\n",
        "  - Undersampling means down sample the majority class to match the minority class (e.g. random sample `200` out `1,000`)\n",
        "  - Oversampling means up sample the minority class to match the majority class (e.g. duplicate the `200` five times to match `1,000`) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6sO2B1Qz0uYN",
        "colab_type": "text"
      },
      "source": [
        "<!--## How to handle imbalanced data in theory?-->\n",
        "\n",
        "- Undersampling and oversampling both have their pros and cons\n",
        "  - Undersampling: you lose part of your dataset, if you have a __small__ dataset, then you should not use that;\n",
        "  - Oversampling: if we do _simple_ oversampling, by duplicating the data, we introduce error in the dataset\n",
        "    - Emphasize on patterns that might not be truly important in the dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vcsXGwRo1a9J",
        "colab_type": "text"
      },
      "source": [
        "## How to handle imbalanced data in practice?\n",
        "\n",
        "- Syntheic Minority Oversampling TEchnique (SMOTE)\n",
        "  - it is an oversampling technique\n",
        "  - proposed by This Nitesh Chawla, et al. in [“SMOTE: Synthetic Minority Over-sampling Technique.”](https://arxiv.org/abs/1106.1813)\n",
        "- Near Miss Algorithm\n",
        "  - it is an undersampling technique\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qLu491nC2hp-",
        "colab_type": "text"
      },
      "source": [
        "## How SMOTE works?\n",
        "\n",
        "- SMOTE works by selecting examples that are close in the feature space, \n",
        "  - drawing a line between the examples in the feature space and drawing a new sample at a point along that line.\n",
        "- It is recommended that \"The combination of SMOTE and under-sampling performs better than plain under-sampling.\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lqAWVH523LP0",
        "colab_type": "text"
      },
      "source": [
        "<!--## How SMOTE works?-->\n",
        "\n",
        "- generates the synthetic training records by linear interpolation \n",
        "  - such as `s1-5` points below\n",
        "\n",
        "<img src = 'https://www.researchgate.net/profile/Christina_Bogner/publication/322701982/figure/fig1/AS:586787038715904@1516912340346/Illustration-of-SMOTE-Synthetic-points-crosses-denoted-s1-through-s5-generated-by.png' alt = 'SMOTE Illustration' width=\"300\" height=\"300\" />"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZPUt4uLU4Mty",
        "colab_type": "text"
      },
      "source": [
        "## The SMOTE Algorithm\n",
        "\n",
        "- __Step 1__: Setting the minority class set $A$, for each $x \\in A$, the __k-nearest neighbors__ of $x$ are obtained by calculating the __Euclidean distances__ between $x$ and every other sample in set $A$.\n",
        "- __Step 2__: The __sampling rate__ $n$ is set according to the imbalanced proportion. For each $x \\in A$, $n$ examples (i.e $x_1, x_2, …, x_n$) are randomly selected from its k-nearest neighbors, and they construct a new set $A_1$ .\n",
        "- __Step 3__: For each example $x_k \\in A_1$ (k=1, 2, 3…n), the following formula is used to generate a new example:\n",
        "$$x' = x + rand(0, 1) \\times \\mid x - x_k \\mid$$\n",
        "in which $rand(0, 1)$ represents the random number between 0 and 1."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ys8N74A55XU7",
        "colab_type": "text"
      },
      "source": [
        "## SMOTE in Action\n",
        "\n",
        "Now that we are familiar with the technique, let’s look at a worked example for an imbalanced classification problem.\n",
        "\n",
        "We will need the Imbalanced-learn (`imblearn`) Library - which is already installed for you. If you want to install that on your own, you can do that by:\n",
        "```shell\n",
        "  sudo pip install imbalanced-learn\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-_qbmiBm6xHM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# skip this \n",
        "# import warnings\n",
        "# warnings.filterwarnings(\"ignore\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qyv-7_Zv6QrH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# skip this\n",
        "# ! pip install imbalanced-learn==0.5.0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k1ARJDzn0uFa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# check version number\n",
        "import imblearn\n",
        "print(imblearn.__version__)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gwuAvvphz0Nw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import necessary packages  \n",
        "import pandas as pd \n",
        "import matplotlib.pyplot as plt \n",
        "import numpy as np \n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression \n",
        "from sklearn.preprocessing import StandardScaler \n",
        "from sklearn.metrics import confusion_matrix, classification_report \n",
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.under_sampling import NearMiss \n",
        "\n",
        "#\n",
        "%matplotlib inline\n",
        "plt.style.use('ggplot')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2D00fw8A7KbG",
        "colab_type": "text"
      },
      "source": [
        "### Dataset Used \n",
        "\n",
        "<!--The dataset consists of transactions made by credit cards. This dataset has `492` __fraud__ transactions out of `284, 807` transactions. That makes it highly unbalanced, the positive class (frauds) account for __0.172%__ of all transactions.\n",
        "The dataset can be downloaded from [here](https://www.kaggle.com/mlg-ulb/creditcardfraud).-->\n",
        "\n",
        "In this section, we will develop an intuition for the SMOTE by applying it to an __imbalanced binary classification problem__.\n",
        "\n",
        "First, we can use the `make_classification()` function from [scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_classification.html) to create a synthetic binary classification dataset with `50,000` examples and a `5:95` class distribution, with `15` features."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "06lvCFYjySbv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# define dataset\n",
        "X, y = make_classification(n_samples=50000, n_features=15, n_informative = 10, n_redundant=2, n_classes=2,\n",
        "\tn_clusters_per_class=1, weights=[0.95], flip_y=0.3, random_state=2019) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WjhNCGRj-w5c",
        "colab_type": "text"
      },
      "source": [
        "We can then make them into DataFrames (`feature_df` and `target_df`) to review them."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FNEfOjoq9ywL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "feature_df = pd.DataFrame(X)\n",
        "feature_df.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w90WwHc2--i3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "feature_df.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2YecOhJN_I7o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "target_df = pd.Series(y)\n",
        "target_df.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bf0BTOF1_Qxp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "target_df.value_counts()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FvQl-2Tg_ifw",
        "colab_type": "text"
      },
      "source": [
        "We can plot it in a bar chart to see how much the data is imbalanced."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aP7LLtRj_Ush",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "target_df.value_counts().plot(kind='bar');"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ul6AZLdIAkLc",
        "colab_type": "text"
      },
      "source": [
        "We can also look at the `info` of the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LGImeTGK_-hX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "feature_df.info()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KX2Zoa24CTw-",
        "colab_type": "text"
      },
      "source": [
        "Let's look at the descriptive statistics of the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MXc50ByfCcz0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "feature_df.describe()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eWwh9IO3Cpea",
        "colab_type": "text"
      },
      "source": [
        "Let's scale te data so that all the features are in the same range."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W4KexaW8ArHU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "scaled_df = pd.DataFrame(StandardScaler().fit_transform(feature_df))\n",
        "scaled_df.describe()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6OKdd5bNC8SC",
        "colab_type": "text"
      },
      "source": [
        "Now to split the data into training and testing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KbrilX-zCG-o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# split into 80:20 ration \n",
        "X_train, X_test, y_train, y_test = train_test_split(scaled_df.values, target_df.values, test_size = 0.2, random_state = 2019) \n",
        "  \n",
        "# describes info about train and test set \n",
        "print(\"Number transactions X_train dataset: \", X_train.shape) \n",
        "print(\"Number transactions y_train dataset: \", y_train.shape) \n",
        "print(\"Number transactions X_test dataset: \", X_test.shape) \n",
        "print(\"Number transactions y_test dataset: \", y_test.shape) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-OlNdp1fDVBi",
        "colab_type": "text"
      },
      "source": [
        "Now train the model without handling the imbalanced class distribution."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YGRbZa6tDL8O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# logistic regression object \n",
        "lr = LogisticRegression(max_iter=1000) \n",
        "  \n",
        "# train the model on train set \n",
        "lr.fit(X_train, y_train.ravel()) \n",
        "  \n",
        "predictions = lr.predict(X_test) \n",
        "  \n",
        "# print classification report \n",
        "print(classification_report(y_test, predictions)) \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z5LBrBG3DlTB",
        "colab_type": "text"
      },
      "source": [
        "The accuracy comes out to be `84%` but did you notice something strange?\n",
        "\n",
        "The _recall_ of the minority class (`1`) in very low. It proves that the model is more biased towards majority class (`0`). So, it proves that this is not the best model.\n",
        "\n",
        "Now, we will apply the SMOTE techniques and see the changes in the model performance.\n",
        "\n",
        "__NOTE__: we only performs SMOTE, or any other balancing technique, on the __training__ dataset. You should __never__ touch the test data.\n",
        "\n",
        "First, let's look at the original distribution again.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oahJ-eEXDanX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"Before OverSampling, counts of label '1': {}\".format(sum(y_train == 1))) \n",
        "print(\"Before OverSampling, counts of label '0': {} \\n\".format(sum(y_train == 0))) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WXdhNpE2E7U0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sm = SMOTE(random_state = 123) \n",
        "X_train_res, y_train_res = sm.fit_sample(X_train, y_train) \n",
        "  \n",
        "print('After OverSampling, the shape of train_X: {}'.format(X_train_res.shape)) \n",
        "print('After OverSampling, the shape of train_y: {} \\n'.format(y_train_res.shape)) \n",
        "  \n",
        "print(\"After OverSampling, counts of label '1': {}\".format(sum(y_train_res == 1))) \n",
        "print(\"After OverSampling, counts of label '0': {}\".format(sum(y_train_res == 0))) \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0nFWldl-FP-T",
        "colab_type": "text"
      },
      "source": [
        "Look that SMOTE Algorithm has oversampled the instances in the minority and made it equal to majority class (`32,581`). Both categories have equal amount of records. Accordingly, we see the overall size the training set increased from `32,581` to `65,162`. _Note_ that the additional `32,581` instances are synthesized and they are not in the actual data - hence, we introduce error in the dataset and then in our model.\n",
        "\n",
        "Now see the accuracy and recall results after applying SMOTE algorithm (Oversampling)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C38yvAQuFKit",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lr1 = LogisticRegression(max_iter=1000) \n",
        "lr1.fit(X_train_res, y_train_res) \n",
        "y_pred = lr1.predict(X_test) \n",
        "  \n",
        "# print classification report \n",
        "print(classification_report(y_test, y_pred)) \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IPQyizHBGrVX",
        "colab_type": "text"
      },
      "source": [
        "Wait, what? We see a huge descrease in model accuracy from `84%` to `66%`. Nobody likes that kind of drop, correct?\n",
        "\n",
        "That is simply not true. When we had `84%` as the accuracy of the initial model, we are comparing against a threshold of `95%`. Why? \n",
        "\n",
        "Since if we assign all the instances in the testing set to the majority class (`0`), or simply by __random guessing__, we get an accuracy of `95%`. In that sense, our model is <span style=\"color:red\"> __11% worse__ </span> than random guessing. Clearly that is not a good model.\n",
        "\n",
        "Now let's look at the modeling results after resampling. We are comparing against the threshold of `50%` (random guessing, since we have equal amounts of instances in the two classes). Thus, we result in a <span style=\"color:blue\"> __16% increase__ </span> by applying SMOTE. Also, we notice that the recall in the minority class increased by `0.26`, which means our new model is more capable of capturing the pattern(s) in the minority class.\n",
        "\n",
        "SMOTE clearly works in our favor. But still, we introduced error in our dataset, you can observe that in the declined performance in the results toward the majority class.\n",
        "\n",
        "That is why we need to try the NearMiss (undersampling) method."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YnJy68yQJOI9",
        "colab_type": "text"
      },
      "source": [
        "## How NearMiss works?\n",
        "\n",
        "- NearMiss is an under-sampling technique. It aims to balance class distribution by __randomly eliminating__ majority class examples. \n",
        "\n",
        "- To prevent problem of information loss in most under-sampling techniques, __near-neighbor__ methods are widely used.\n",
        "\n",
        "- The basic intuition about the working of near-neighbor methods is as follows:\n",
        "\n",
        "  - __Step 1__: The method first finds the distances between all instances of the majority class and the instances of the minority class. Here, __majority class__ is to be under-sampled.\n",
        "  - __Step 2__: Then, $n$ instances of the majority class that have the __smallest distances__ to the minority class are selected.\n",
        "  - __Step 3__: If there are $k$ instances in the minority class, the nearest method will result in $k \\times n$ instances of the majority class.\n",
        "\n",
        "Key difference to SMOTE: NearMiss works on both the majority and minority classes, where as SMOTE only works on the minority class."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hv0e8-uCZRNq",
        "colab_type": "text"
      },
      "source": [
        "<!-- # How NearMiss works? -->\n",
        "- For finding $n$ closest instances in the majority class, there are several variations of applying NearMiss Algorithm :\n",
        "\n",
        "  - Version 1: It selects samples of the majority class for which _average_ distances to the $k$ __closest__ instances of the minority class is _smallest_.\n",
        "  - Version 2: It selects samples of the majority class for which _average_ distances to the $k$ __farthest__ instances of the minority class is _smallest_.\n",
        "  - Version 3: It works in 2 steps. \n",
        "    - Firstly, for each minority class instance, their $M$ nearest-neighbors will be stored. \n",
        "    - Secondly, the majority class instances are selected for which the average distance to the $N$ nearest-neighbors is the _largest_.\n",
        "\n",
        "Let's look at the original class distribution again."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mxHaj1khGKv0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"Before Undersampling, counts of label '1': {}\".format(sum(y_train == 1))) \n",
        "print(\"Before Undersampling, counts of label '0': {} \\n\".format(sum(y_train == 0))) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kxSaRkC_aayw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# apply near miss \n",
        "\n",
        "nr = NearMiss(random_state=123) \n",
        "  \n",
        "X_train_miss, y_train_miss = nr.fit_sample(X_train, y_train) \n",
        "  \n",
        "print('After Undersampling, the shape of train_X: {}'.format(X_train_miss.shape)) \n",
        "print('After Undersampling, the shape of train_y: {} \\n'.format(y_train_miss.shape)) \n",
        "  \n",
        "print(\"After Undersampling, counts of label '1': {}\".format(sum(y_train_miss == 1))) \n",
        "print(\"After Undersampling, counts of label '0': {}\".format(sum(y_train_miss == 0))) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2bcrXXgqaiQV",
        "colab_type": "text"
      },
      "source": [
        "The NearMiss Algorithm has undersampled the majority instances and made it equal to majority class. Here, the majority class has been reduced to the total number of minority class (`14,838`), so that both classes will have equal number of records. __Note__ that by doing so, we lost $ 32,581 - 14,838 = 17,743 $ intances in the majority class. Hence, we will observe a loss in the model performance.\n",
        "\n",
        "Let's check out the modeling results after applying the NearMiss technique on the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IuZhpZsGaeTZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# train the model on train set \n",
        "lr2 = LogisticRegression(max_iter=1000) \n",
        "lr2.fit(X_train_miss, y_train_miss) \n",
        "y_pred = lr2.predict(X_test) \n",
        "  \n",
        "# print classification report \n",
        "print(classification_report(y_test, y_pred)) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NGSAZa8ebczh",
        "colab_type": "text"
      },
      "source": [
        "Similarly, we observe a decline in the modeling performance (e.g. acc: `84%` -> `64%`). But using the same logic as we analyzing the SMOTE results, we know that now we have an edge of $+14\\%$ over random guessing. \n",
        "\n",
        "In addition, by comparing the SMOTE and NearMiss results, we know that the SMOTE leads to better results - it is because we did not lose much data as we do in the NearMiss scenario. Thus, we can go with the SMOTE technique here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UZNelfG0dAWD",
        "colab_type": "text"
      },
      "source": [
        "## Conclusion\n",
        "\n",
        "Now we understand how oversampling (__SMOTE__) and undersampling (__NearMiss__) work, and how do we use them in balancing the imbalanced dataset. \n",
        "\n",
        "They both have their own pros and cons, so you should choose them wisely. Additionally, researchers and data scientists are working on merging the two, or in general oversampling and undersampling together, to battle with imbalanced datasets. If you are interested, please [let me know](mailto:jtao@fairfield.edu)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "odLM93c5eNtr",
        "colab_type": "text"
      },
      "source": [
        "## References\n",
        "\n",
        "1. [Imbalance-Learn examples](http://glemaitre.github.io/imbalanced-learn/auto_examples/index.html)\n",
        "2. [ML | Handling Imbalanced Data with SMOTE and Near Miss Algorithm in Python](https://www.geeksforgeeks.org/ml-handling-imbalanced-data-with-smote-and-near-miss-algorithm-in-python/)\n",
        "3. [SMOTE Oversampling for Imbalanced Classification with Python](https://machinelearningmastery.com/smote-oversampling-for-imbalanced-classification/)\n",
        "4. [Survey of resampling techniques for improving classification performance in unbalanced datasets - research article](https://arxiv.org/pdf/1608.06048.pdf)\n",
        "5. [Undersampling Algorithms for Imbalanced Classification](https://machinelearningmastery.com/undersampling-algorithms-for-imbalanced-classification/)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pLWSHV-bbVjX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}